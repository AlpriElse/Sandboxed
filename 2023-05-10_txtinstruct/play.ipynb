{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neuml/txtinstruct/blob/master/examples/01_Introducing_txtinstruct.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neuml/txtai\n",
      "  Cloning https://github.com/neuml/txtai to /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-req-build-hkzry3lc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neuml/txtai /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-req-build-hkzry3lc\n",
      "  Resolved https://github.com/neuml/txtai to commit 6b4979e670ad90ae3f6aaeda8e6accd993ae96a3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/neuml/txtinstruct\n",
      "  Cloning https://github.com/neuml/txtinstruct to /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-req-build-v767uubx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neuml/txtinstruct /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-req-build-v767uubx\n",
      "  Resolved https://github.com/neuml/txtinstruct to commit eb9fd98e7afe8aa78a10f9f0bc2878f84eba1a30\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss-cpu>=1.7.1.post2 (from txtai==5.6.0)\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-macosx_10_9_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.18.4 (from txtai==5.6.0)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "Collecting pyyaml>=5.3 (from txtai==5.6.0)\n",
      "  Using cached PyYAML-6.0-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "Collecting torch>=1.6.0 (from txtai==5.6.0)\n",
      "  Downloading torch-2.0.1-cp39-none-macosx_10_9_x86_64.whl (143.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.22.0 (from txtai==5.6.0)\n",
      "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.8.0 (from txtinstruct==0.1.0)\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.48.0 (from txtinstruct==0.1.0)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading pyarrow-12.0.0-cp39-cp39-macosx_10_14_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pandas (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached pandas-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Collecting requests>=2.19.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting multiprocess (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-macosx_10_9_x86_64.whl (360 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from datasets>=2.8.0->txtinstruct==0.1.0) (23.1)\n",
      "Collecting responses<0.19 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting filelock (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->txtai==5.6.0) (4.5.0)\n",
      "Collecting sympy (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.22.0->txtai==5.6.0)\n",
      "  Downloading regex-2023.5.5-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.22.0->txtai==5.6.0)\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (124 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached yarl-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl (65 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->txtai==5.6.0)\n",
      "  Using cached MarkupSafe-2.1.2-cp39-cp39-macosx_10_9_x86_64.whl (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->datasets>=2.8.0->txtinstruct==0.1.0) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->txtai==5.6.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.8.0->txtinstruct==0.1.0) (1.16.0)\n",
      "Building wheels for collected packages: txtai, txtinstruct\n",
      "  Building wheel for txtai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for txtai: filename=txtai-5.6.0-py3-none-any.whl size=171760 sha256=5aa1e4e730b9e2790e9d76dcce35894f089df3274367356391f07d3fe0aaa70f\n",
      "  Stored in directory: /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-ephem-wheel-cache-l3ky5j3q/wheels/1c/21/76/fe34260886f366a2a7f440e42722b0a60b84cf8e466201b22a\n",
      "  Building wheel for txtinstruct (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for txtinstruct: filename=txtinstruct-0.1.0-py3-none-any.whl size=13450 sha256=b218cb84760abbf38f87cc114e5e4e0f2ca07b7d6b5b08dd5da42f2442ffd112\n",
      "  Stored in directory: /private/var/folders/kt/7gdyyk6d75lbw1xwdqkrbrjw0000gn/T/pip-ephem-wheel-cache-l3ky5j3q/wheels/0c/d2/22/b6d761c4f666b7eb8d856d1d81167a60d21ba91aec119a2b21\n",
      "Successfully built txtai txtinstruct\n",
      "Installing collected packages: tokenizers, pytz, mpmath, faiss-cpu, xxhash, urllib3, tzdata, tqdm, sympy, regex, pyyaml, numpy, networkx, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, pyarrow, pandas, multiprocess, jinja2, aiosignal, torch, responses, huggingface-hub, aiohttp, transformers, txtai, datasets, txtinstruct\n",
      "Successfully installed MarkupSafe-2.1.2 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 datasets-2.12.0 dill-0.3.6 faiss-cpu-1.7.4 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 networkx-3.1 numpy-1.24.3 pandas-2.0.1 pyarrow-12.0.0 pytz-2023.3 pyyaml-6.0 regex-2023.5.5 requests-2.30.0 responses-0.18.0 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.29.0 txtai-5.6.0 txtinstruct-0.1.0 tzdata-2023.3 urllib3-2.0.2 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/neuml/txtai git+https://github.com/neuml/txtinstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.9/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alprielse/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.27k/5.27k [00:00<00:00, 4.44MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.36k/2.36k [00:00<00:00, 14.1MB/s]\n",
      "Downloading readme: 100%|██████████| 7.67k/7.67k [00:00<00:00, 27.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to /Users/alprielse/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 30.3MB [00:00, 54.1MB/s]/2 [00:00<?, ?it/s]\n",
      "Downloading data: 4.85MB [00:00, 39.5MB/s]/2 [00:01<00:01,  1.49s/it]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1063.87it/s]\n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/alprielse/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset generator/default to /Users/alprielse/.cache/huggingface/datasets/generator/default-4dafeecd279c8f2a/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /Users/alprielse/.cache/huggingface/datasets/generator/default-4dafeecd279c8f2a/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m generator \u001b[39m=\u001b[39m StatementGenerator()\n\u001b[0;32m---> 10\u001b[0m model, tokenizer \u001b[39m=\u001b[39m generator(\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoogle/flan-t5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     dataset,\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39msequence-sequence\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m16\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     logging_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/txtinstruct/models/statement.py:40\u001b[0m, in \u001b[0;36mStatementGenerator.__call__\u001b[0;34m(self, base, data, task, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m trainer \u001b[39m=\u001b[39m HFTrainer()\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m trainer(base, train, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/txtai/pipeline/train/hftrainer.py:69\u001b[0m, in \u001b[0;36mHFTrainer.__call__\u001b[0;34m(self, base, train, validation, columns, maxlength, stride, task, prefix, metrics, tokenizers, checkpoint, **args)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mBuilds a new model using arguments.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    (model, tokenizer)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Parse TrainingArguments\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse(args)\n\u001b[1;32m     71\u001b[0m \u001b[39m# Set seed for model reproducibility\u001b[39;00m\n\u001b[1;32m     72\u001b[0m set_seed(args\u001b[39m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/txtai/pipeline/train/hftrainer.py:152\u001b[0m, in \u001b[0;36mHFTrainer.parse\u001b[0;34m(self, updates)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m# Apply custom arguments\u001b[39;00m\n\u001b[1;32m    150\u001b[0m args\u001b[39m.\u001b[39mupdate(updates)\n\u001b[0;32m--> 152\u001b[0m \u001b[39mreturn\u001b[39;00m TrainingArguments(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/training_args.py:1333\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1328\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1334\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1335\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[1;32m   1336\u001b[0m ):\n\u001b[1;32m   1337\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1340\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1344\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[1;32m   1350\u001b[0m ):\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/training_args.py:1697\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1697\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/src/sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/training_args.py:1613\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1611\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mPyTorch: setting up devices\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1612\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(check_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1613\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   1614\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1615\u001b[0m     )\n\u001b[1;32m   1616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_cuda:\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m PartialState(cpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, backend\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mddp_backend)\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from txtinstruct.models import StatementGenerator\n",
    "\n",
    "# Load SQuAD dataset\n",
    "dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# Train model\n",
    "generator = StatementGenerator()\n",
    "model, tokenizer = generator(\n",
    "    \"google/flan-t5-small\",\n",
    "    dataset,\n",
    "    \"sequence-sequence\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=128 // 16,\n",
    "    num_train_epochs=0.1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
