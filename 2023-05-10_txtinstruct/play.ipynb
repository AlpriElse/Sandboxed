{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neuml/txtinstruct/blob/master/examples/01_Introducing_txtinstruct.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://readonly:****@private.hubteam.com/pypi/simple/\n",
      "Collecting git+https://github.com/neuml/txtai\n",
      "  Cloning https://github.com/neuml/txtai to /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-req-build-n8fgpcg0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neuml/txtai /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-req-build-n8fgpcg0\n",
      "  Resolved https://github.com/neuml/txtai to commit 6b4979e670ad90ae3f6aaeda8e6accd993ae96a3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/neuml/txtinstruct\n",
      "  Cloning https://github.com/neuml/txtinstruct to /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-req-build-e3g2rcfw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neuml/txtinstruct /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-req-build-e3g2rcfw\n",
      "  Resolved https://github.com/neuml/txtinstruct to commit eb9fd98e7afe8aa78a10f9f0bc2878f84eba1a30\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss-cpu>=1.7.1.post2 (from txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/25/5f/54c78d2897138afc569cb8af98499e87e3263b20db718a2d264e1e591cda/faiss_cpu-1.7.4-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.18.4 (from txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/54/41/fb17c1d48a574c50422ff3f1b17ed979b755adc6ed291c4a44a76e226c67/numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.3 (from txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/67/d4/b95266228a25ef5bd70984c08b4efce2c035a4baa5ccafa827b266e3dc36/PyYAML-6.0-cp39-cp39-macosx_11_0_arm64.whl (173 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.0/174.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.6.0 (from txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/3c/67/7e19ebc15430f7385baee359383744c03d3600b51def9b399d0b8686e892/torch-2.0.1-cp39-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.22.0 (from txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/45/e4/4914b11df70954d95a7c36b74bf9010c8594fcec960471479449b0deb4f7/transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.8.0 (from txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/fb/1c/85a22f3fa02dce5403094c5dbce494d62343b5a7e518bdf6e4200dda7337/datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.48.0 (from txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/e6/02/a2cff6306177ae6bc73bc0665065de51dfb3b9db7373e122e2735faf0d97/tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/3c/5d/0c3d8c8cfe53126fd2f79748da254ae05c99b2b563bfc27a9f5c34e48764/pyarrow-12.0.0-cp39-cp39-macosx_11_0_arm64.whl (22.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.7/22.7 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/be/e3/a84bf2e561beed15813080d693b4b27573262433fced9c1d1fea59e60553/dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/55/4f/934c0be7d9d50f9c0ca306281e3fc306b17b086c672deed55c6fe55ab2c6/pandas-2.0.1-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.19.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/96/80/034ffeca15c0f4e01b7b9c6ad0fb704b44e190cde4e757edbd60be404c41/requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/a0/97/7bbaddb39ad314c1e1cc9253eca1b5cc3c4e9bc08279bc003608191085ac/xxhash-3.2.0-cp39-cp39-macosx_11_0_arm64.whl (31 kB)\n",
      "Collecting multiprocess (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/e1/a4/98f97b3aaa84b8be6702bad22ee9893dd506ed7c9dbfdad481793677066b/multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/ec/4e/397b234a369df06ec782666fcdf9791d125ca6de48729814b381af8c6c03/fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/2a/a9/4183ba5557c40aa5e3e94360ae945c159234b968572bcf3f96ef97ac1bc7/aiohttp-3.8.4-cp39-cp39-macosx_11_0_arm64.whl (338 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.3/338.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/58/34/c57b951aecd0248845932c1cfc15721237c50e463f26b0536673bcb76f4f/huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from datasets>=2.8.0->txtinstruct==0.1.0) (23.1)\n",
      "Collecting responses<0.19 (from datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting filelock (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/ad/73/b094a662ae05cdc4ec95bc54e434e307986a5de5960166b8161b7c1373ee/filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->txtai==5.6.0) (4.5.0)\n",
      "Collecting sympy (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/d2/05/e6600db80270777c4a64238a98d442f0fd07cc8915be2a1c16da7f2b9e74/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/a8/05/9d4f9b78ead6b2661d6e8ea772e111fc4a9fbd866ad0c81906c11206b55e/networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers>=4.22.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/f0/3c/99f8edd0feb2e97117d6e797a99455529d9ff725e45b9b902b0b2c1d02d2/regex-2023.5.5-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.9/288.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.22.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/ea/08/fa8ed4b189e184f855cda96e0076d56200b675c2f68c34b674a710985dcf/tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/f0/eb/fcb708c7bf5056045e9e98f62b93bd7467eb718b0202e7698eb11d66416c/attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/ac/7f/62d5dff4e9cb993e4b0d4ea78a74cc84d7d92120879529e0ce0965765936/charset_normalizer-3.1.0-cp39-cp39-macosx_11_0_arm64.whl (122 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.0/123.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/dd/bd/cce218536b377efbee70d8680a97fd282f4e1e9f7ebff95c4ea28deef87a/multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/14/5b/a0bf4a601ddf4073ae0dcd66a90708aaa944a4ad0addf777d9f1fcd6f4f0/yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/f0/4b/06867dc936ec4238cf86e9eeff11e32b262302b65158acf38cac04362d29/frozenlist-1.3.3-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/4b/1d/f8383ef593114755429c307449e7717b87044b3bcd5f7860b89b1f759e34/urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests>=2.19.0->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/9d/19/59961b522e6757f0c9097e4493fa906031b95b3ebe9360b2c3083561a6b4/certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/82/e3/4efcd74f10a7999783955aec36386f71082e6d7dafcc06b77b9df72b325a/MarkupSafe-2.1.2-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->datasets>=2.8.0->txtinstruct==0.1.0) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/7f/99/ad6bd37e748257dd70d6f85d916cafe79c0b0f5e2e95b11f7fbc82bf3110/pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets>=2.8.0->txtinstruct==0.1.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy->torch>=1.6.0->txtai==5.6.0)\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.8.0->txtinstruct==0.1.0) (1.16.0)\n",
      "Building wheels for collected packages: txtai, txtinstruct\n",
      "  Building wheel for txtai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for txtai: filename=txtai-5.6.0-py3-none-any.whl size=171760 sha256=92e6bbefab056fbc3c3f75c0621de8a39de9c72a794d8e6fc5f5ad53d9cc2e45\n",
      "  Stored in directory: /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-ephem-wheel-cache-cs4yxd_i/wheels/1c/21/76/fe34260886f366a2a7f440e42722b0a60b84cf8e466201b22a\n",
      "  Building wheel for txtinstruct (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for txtinstruct: filename=txtinstruct-0.1.0-py3-none-any.whl size=13450 sha256=23f2be8c4739ba85141056c865f86cffb61073fa558c57d213c8fe516be7e92a\n",
      "  Stored in directory: /private/var/folders/rl/1_tcn9md02n9srcxdxxy3dk80000gq/T/pip-ephem-wheel-cache-cs4yxd_i/wheels/0c/d2/22/b6d761c4f666b7eb8d856d1d81167a60d21ba91aec119a2b21\n",
      "Successfully built txtai txtinstruct\n",
      "Installing collected packages: tokenizers, pytz, mpmath, faiss-cpu, xxhash, urllib3, tzdata, tqdm, sympy, regex, pyyaml, numpy, networkx, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, pyarrow, pandas, multiprocess, jinja2, aiosignal, torch, responses, huggingface-hub, aiohttp, transformers, txtai, datasets, txtinstruct\n",
      "Successfully installed MarkupSafe-2.1.2 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 datasets-2.12.0 dill-0.3.6 faiss-cpu-1.7.4 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.13 networkx-3.1 numpy-1.24.3 pandas-2.0.1 pyarrow-12.0.0 pytz-2023.3 pyyaml-6.0 regex-2023.5.5 requests-2.30.0 responses-0.18.0 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.29.0 txtai-5.6.0 txtinstruct-0.1.0 tzdata-2023.3 urllib3-2.0.2 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/neuml/txtai git+https://github.com/neuml/txtinstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://readonly:****@private.hubteam.com/pypi/simple/\n",
      "Collecting accelerate\n",
      "  Downloading https://build.hubteam.com/pypi-mirror/packages/fe/03/08a2f414de4ce11fae93a079e4184a7397f34d956c894359ed801b89dae0/accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.9/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelse/personal-src/Sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.27k/5.27k [00:00<00:00, 8.79MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.36k/2.36k [00:00<00:00, 13.3MB/s]\n",
      "Downloading readme: 100%|██████████| 7.67k/7.67k [00:00<00:00, 22.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to /Users/aelse/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 30.3MB [00:00, 100MB/s]0/2 [00:00<?, ?it/s]\n",
      "Downloading data: 4.85MB [00:00, 52.1MB/s]                   .30s/it]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1262.01it/s]\n",
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/aelse/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset generator/default to /Users/aelse/.cache/huggingface/datasets/generator/default-fd247e65caa0f13d/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /Users/aelse/.cache/huggingface/datasets/generator/default-fd247e65caa0f13d/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 1.19MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 2.32MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 9.41MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 20.5MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 1.17MB/s]\n",
      "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]/Users/aelse/personal-src/Sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:04<00:00, 70.4MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 107kB/s]\n",
      "/Users/aelse/personal-src/Sandboxed/2023-05-10_txtinstruct/.venv/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/69 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 69/69 [25:59<00:00, 22.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1559.3506, 'train_samples_per_second': 5.618, 'train_steps_per_second': 0.044, 'train_loss': 2.1997397602468296, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from txtinstruct.models import StatementGenerator\n",
    "\n",
    "# Load SQuAD dataset\n",
    "dataset = load_dataset(\"squad\", split=\"train\")\n",
    "\n",
    "# Train model\n",
    "generator = StatementGenerator()\n",
    "model, tokenizer = generator(\n",
    "    \"google/flan-t5-small\",\n",
    "    dataset,\n",
    "    \"sequence-sequence\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=128 // 16,\n",
    "    num_train_epochs=0.1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is txtai a platform for?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from txtai.pipeline import Sequences\n",
    "\n",
    "# Load statement generation model\n",
    "statements = Sequences((model, tokenizer))\n",
    "\n",
    "# Run example prompt\n",
    "statements(\"\"\"Generate a question using the context below.\n",
    "### Context:\n",
    "txtai is an open-source platform for semantic search and workflows powered by language models.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 106454.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 201649.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from txtai.embeddings import Embeddings\n",
    "from txtinstruct.data import DatasetBuilder\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = Embeddings()\n",
    "embeddings.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia\")\n",
    "\n",
    "# Query templates\n",
    "templates = [\n",
    "    \"Tell me about {text}\",\n",
    "    \"Give an explanation on {text}\",\n",
    "    \"Provide a quick summary on {text}\",\n",
    "    \"Explain {text} in simple terms\",\n",
    "    \"Describe {text}\"\n",
    "]\n",
    "\n",
    "# Build dataset\n",
    "builder = DatasetBuilder(Sequences(\"google/flan-t5-base\"), statements, templates)\n",
    "builder(\n",
    "    embeddings.search(\"SELECT id, text FROM txtai WHERE similar('machine learning') AND percentile >= 0.99 LIMIT 5\"),\n",
    "    5,\n",
    "    \"data.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
